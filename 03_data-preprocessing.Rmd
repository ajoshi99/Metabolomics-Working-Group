---
title: "3. Data pre-processing"
author: "Anu Joshi"
date: "May 18, 2019"
output:
  html_document:
    keep_md: yes
    theme: united
    highlight: tango
---

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

library(readr)                  # faster reading and writing csv files
library(dplyr)                  # data manipulation
library(magrittr)               # pipe operator
library(tibble)                 # row and data manipulation
```
<br>


Dataset has samples (459 observations) in rows and covariates (1308 metabolites + 6 variables) in columns. 
```{r load-data, results='hide', message=FALSE}
df0 = read_csv("data/2_metabolite-clean.csv") 
```


_Note_: If you would like to remove data that has >30% missing values, use the filtered dataset. 
```{r load-data, results='hide', message=FALSE}
df0 = read_csv("data/2_metabolite-clean.csv") 


# extract all covariates to merge later
# 459 observations of 6 variables
df_var = df0[, c(1:6)]


# remove all covariates for the preprocessing steps
# use Assay as sample identifier in rownames
# 459 observation of 1308 metabolites
df = df0[, -c(2:6)] %>%
    column_to_rownames(var = "Assay")
```
<br> 


**1. Data imputation**    
Missing values in the dataset can be due to two major reasons - either the metabolites are absent or they are below the detection limit. The missing data is Missing Not At Random (MNAR) so we should impute the missing values instead of completely removing it from the analysis. <br>
```{r missing-df, results='hide'}
sum(is.na(df))
```
The dataset has 179,520 missing datapoints and it is therefore not a statistically viable option to remove the missing data without introducing censoring bias. 
<br> 


Using the minimum value of the metabolite divided by the square root of 2 for data imputation. <br>
```{r impute1, results='hide'}
# Step 1: create a dummy copy of the dataset
df_impute <- df

# Step 2: impute missing values with minimum/sqrt(2) 
for(i in 1:ncol(df_impute)){
  df_impute[is.na(df_impute[,i]), i] <- min(df_impute[, i]/sqrt(2), na.rm = TRUE)
}

# Step 3: check for missing values
# verify that all NAs have been replaced
sum(is.na(df_impute))

# Step 4: Save file
df_impute0 = df_impute %>% 
    rownames_to_column(var = "Assay") %>%
    merge(df_var, ., by = "Assay")
write_csv(df_impute0, "data/3_metabolite-Imp.csv")
```
<br>
Other methods to consider for data imputation:      
* Use the minimum value/minimum metabolite abundance value                  
* kNN (k-Nearest Neighbors)         
* [No-Skip kNN algorithm](https://link.springer.com/article/10.1007/s11306-018-1451-8) to estimate missing metabolite abundances  
<br>


**2. Volume Normalization**       

We have one sample (RW10351 M) that had their sample extracted at a 60ul while the rest of the samples were measured at 85ul. This can affect the overall abundance value produced by the LC-MS analysis. All normalization methods assume that the signal intensities scale linearly with the metabolite concentration. We are extending this idea and assuming that the metabolite signal intensities scale linearly with the sample volume. 
```{r normalize-data}
# Step 1: create a dummy copy of the dataset
df_impute_scale <- df_impute

# Step 2: Volume Normalizatin
df_impute_scale["RW10351 M", ] <- (df_impute_scale["RW10351 M", ]/60)*85

# Step 3: Save file
df_impute_scale0 = df_impute_scale %>% 
    rownames_to_column(var = "Assay") %>%
    merge(df_var, ., by = "Assay")
write_csv(df_impute_scale0, "data/3_metabolite-VolNormImp.csv")
```
